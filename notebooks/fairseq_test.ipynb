{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "from torch import nn\n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data import Dataset, DataLoader, random_split, ConcatDataset\n",
    "\n",
    "import fairseq \n",
    "import sequitur as seq\n",
    "from sequitur.models import LSTM_AE\n",
    "\n",
    "\n",
    "import os \n",
    "import sys\n",
    "sys.path.append('/Users/ivan_zorin/Documents/DEV/code/ntl/')\n",
    "\n",
    "from data.data import SGCCDataset\n",
    "import models\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28778 3612\n"
     ]
    }
   ],
   "source": [
    "data_path = '/Users/ivan_zorin/Documents/DEV/data/sgcc/data.csv'\n",
    "\n",
    "normal_data = SGCCDataset(data_path, label=0, nan_ratio=0.5)\n",
    "anomal_data = SGCCDataset(data_path, label=1, nan_ratio=1.0)\n",
    "\n",
    "print(len(normal_data), len(anomal_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preparation for sequitur\n",
    "train_data = [torch.tensor(sample[1]) for sample in normal_data]\n",
    "test_data = [torch.tensor(sample[1]) for sample in anomal_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1034, 1)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM_AE(\n",
       "  (encoder): Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): LSTM(1, 512, batch_first=True)\n",
       "      (1): LSTM(512, 512, batch_first=True)\n",
       "      (2): LSTM(512, 256, batch_first=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): LSTM(256, 512, batch_first=True)\n",
       "      (1-2): 2 x LSTM(512, 512, batch_first=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_kwargs = {\n",
    "    'input_dim': 1,\n",
    "    'encoding_dim': 256,\n",
    "    'h_dims': [512, 512],\n",
    "    'h_activ': F.relu,\n",
    "    'out_activ': F.relu\n",
    "}\n",
    "\n",
    "# encoding_dim = 256\n",
    "\n",
    "model = LSTM_AE(**model_kwargs)\n",
    "model\n",
    "\n",
    "\n",
    "x = train_data[0]\n",
    "out = model(x)\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_kwargs = {\n",
    "    'input_dim': 1,\n",
    "    'encoding_dim': 256,\n",
    "    'h_dims': [512, 512],\n",
    "    'h_activ': 'relu',\n",
    "    'out_activ': 'relu'\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM_AE(\n",
       "  (encoder): Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): LSTM(1, 512, batch_first=True)\n",
       "      (1): LSTM(512, 512, batch_first=True)\n",
       "      (2): LSTM(512, 256, batch_first=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): LSTM(256, 512, batch_first=True)\n",
       "      (1-2): 2 x LSTM(512, 512, batch_first=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = SequiturLSTMAE(model2_kwargs)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256]), torch.Size([1034, 1]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].shape, out[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder, decoder, embs, losses = seq.quick_train(LSTM_AE, train_data, encoding_dim=encoding_dim, verbose=True, lr=1e-05, epochs=100, denoise=True, kwargs=model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_config = '/Users/ivan_zorin/Documents/DEV/code/ntl/configs/trainer_debug.yaml'\n",
    "path_config = '/Users/ivan_zorin/Documents/DEV/code/ntl/configs/local_pathes.yaml'\n",
    "\n",
    "config = load_config(experiment_config, path_config)\n",
    "\n",
    "device = config.device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_dataset = SGCCDataset(path=config.data_path, label=0, scale=config.scale, nan_ratio=config.nan_ratio)\n",
    "anomal_dataset = SGCCDataset(path=config.data_path, label=1, scale=config.scale)\n",
    "\n",
    "train_data, val_data, test_normal_data = random_split(normal_dataset, [len(normal_dataset) - 2*len(anomal_dataset), len(anomal_dataset), len(anomal_dataset)])\n",
    "test_data = ConcatDataset([test_normal_data, anomal_dataset])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=config.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(test_data, batch_size=config.batch_size, shuffle=config.debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "y, x, _ = batch\n",
    "x = x.to(device)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequiturLSTMAE(\n",
       "  (encoder): Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): LSTM(1, 512, batch_first=True)\n",
       "      (1): LSTM(512, 512, batch_first=True)\n",
       "      (2): LSTM(512, 256, batch_first=True)\n",
       "    )\n",
       "    (h_activ): ReLU()\n",
       "    (out_activ): ReLU()\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): LSTM(256, 512, batch_first=True)\n",
       "      (1-2): 2 x LSTM(512, 512, batch_first=True)\n",
       "    )\n",
       "    (h_activ): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SequiturLSTMAE(LSTM_AE):\n",
    "    def __init__(self, input_dim, encoding_dim, h_dims=[], h_activ=nn.Sigmoid(), out_activ=nn.Tanh()):\n",
    "\n",
    "        h_activ = getattr(nn, h_activ)()\n",
    "        out_activ = getattr(nn, out_activ)()\n",
    "        \n",
    "        super().__init__(input_dim, encoding_dim, h_dims, h_activ, out_activ)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        seq_len = x.shape[1]\n",
    "        print(x.shape)\n",
    "        z = self.encoder(x)\n",
    "        x = self.decoder(z, seq_len)\n",
    "        \n",
    "        return z, x\n",
    "\n",
    "\n",
    "\n",
    "model = SequiturLSTMAE(**config.model_kwargs).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (layers): ModuleList(\n",
       "    (0): LSTM(1, 512, batch_first=True)\n",
       "    (1): LSTM(512, 512, batch_first=True)\n",
       "    (2): LSTM(512, 256, batch_first=True)\n",
       "  )\n",
       "  (h_activ): ReLU()\n",
       "  (out_activ): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0345,  ..., 0.0198, 0.0200, 0.0106],\n",
       "        [0.0000, 0.0000, 0.0345,  ..., 0.0191, 0.0192, 0.0110],\n",
       "        [0.0000, 0.0000, 0.0344,  ..., 0.0197, 0.0198, 0.0106],\n",
       "        ...,\n",
       "        [0.0000, 0.0000, 0.0344,  ..., 0.0196, 0.0197, 0.0107],\n",
       "        [0.0000, 0.0000, 0.0345,  ..., 0.0199, 0.0200, 0.0106],\n",
       "        [0.0000, 0.0000, 0.0344,  ..., 0.0193, 0.0194, 0.0108]],\n",
       "       grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1034, 1])\n"
     ]
    }
   ],
   "source": [
    "z, x_hat = model(x)\n",
    "\n",
    "# FIXME problem with dimensions in decoder's output. probably x_hat needs to be reshaped like [batch, len, fetaures]. currently it's as [batch * len, features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 256])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8272, 1])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 1])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.decoder.dense_matrix.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
