{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "import os\n",
    "import sys  \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    \n",
    "if IN_COLAB:\n",
    "    !pip install lightning \n",
    "    import lightning.pytorch as pl\n",
    "    DATA_PATH = '/content/drive/MyDrive/4. MODELS/NTL/data/data.csv'\n",
    "\n",
    "else:\n",
    "    sys.path.append('/Users/ivan_zorin/Documents/AIRI/code/ntl/')\n",
    "    DATA_PATH = '/Users/ivan_zorin/Documents/AIRI/data/sgcc/data.csv'\n",
    "    from data.data import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(filepath):\n",
    "    \"\"\"## Saving \"flags\" \"\"\"\n",
    "\n",
    "    df_raw = pd.read_csv(filepath, index_col=0)\n",
    "    flags = df_raw.FLAG.copy()\n",
    "\n",
    "    df_raw.drop([\"FLAG\"], axis=1, inplace=True)\n",
    "\n",
    "    \"\"\"## Sorting\"\"\"\n",
    "\n",
    "    df_raw = df_raw.T.copy()\n",
    "    df_raw.index = pd.to_datetime(df_raw.index)\n",
    "    df_raw.sort_index(inplace=True, axis=0)\n",
    "    df_raw = df_raw.T.copy()\n",
    "    df_raw[\"FLAG\"] = flags\n",
    "    return df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 100\n",
    "BATCH_SIZE = 16\n",
    "LR = 10 ** -5\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGCCDataset(Dataset):\n",
    "    def __init__(self, path, label=None, year=None):\n",
    "        super(SGCCDataset).__init__()\n",
    "        self.path = path\n",
    "        self.label = label\n",
    "        # loading dataset\n",
    "        self.data = self._get_dataset()\n",
    "        self.labels = self.data['FLAG'].to_numpy() # class labels\n",
    "        self.data = self._filter_by_label(self.data, self.label) # extracting data of only selected class\n",
    "        self.data = self.data.drop('FLAG', axis=1)\n",
    "        self._fill_na_() # filling NaN in consumption\n",
    "        self.consumers = self.data.reset_index()['CONS_NO'].to_list() # names of consumers\n",
    "\n",
    "        if year:\n",
    "            #TODO: slice data to have only selected year\n",
    "            # transpose raw_data and pick year in index\n",
    "            pass\n",
    "        \n",
    "        self.length = self.data.shape[0]\n",
    "        self.data = self.data.to_numpy()\n",
    "\n",
    "    def _filter_by_label(self, data, label):\n",
    "        if label in ('normal', 0):\n",
    "            data = data[data['FLAG'] == 0]\n",
    "        elif label in ('anomal', 1):\n",
    "            data = data[data['FLAG'] == 1]\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _fill_na_(self):\n",
    "        # filling with zeros\n",
    "        self.data.fillna(0, inplace=True)\n",
    "\n",
    "    def _get_dataset(self):\n",
    "        return get_dataset(self.path)\n",
    "\n",
    "    def _get_item(self, idx):\n",
    "        return (self.labels[idx], self.data[idx, :], self.consumers[idx])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self._get_item(idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMAE(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=[64], n_lstms=1, activation_fn=nn.ReLU, **lstm_kwargs):\n",
    "        super(LSTMAE, self).__init__()\n",
    "        assert n_lstms == len(hidden_size)\n",
    "\n",
    "        # self.activation_fn = activation_fn\n",
    "\n",
    "        # encoder\n",
    "        encoder, decoder = [], []\n",
    "        self.enc_dims = [input_size] + hidden_size\n",
    "        self.dec_dims = self.enc_dims[::-1]\n",
    "\n",
    "        for i in range(len(self.enc_dims) - 1):\n",
    "            encoder += [nn.LSTM(input_size=self.enc_dims[i], hidden_size=self.enc_dims[i+1], num_layers=1, batch_first=True, **lstm_kwargs)]\n",
    "\n",
    "            decoder += [nn.LSTM(input_size=self.dec_dims[i], hidden_size=self.dec_dims[i+1], num_layers=1, batch_first=True, **lstm_kwargs)]\n",
    "\n",
    "        self.encoder = nn.Sequential(*encoder)\n",
    "        self.decoder = nn.Sequential(*decoder)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, L, H = x.shape # Batch x Length x Feature size\n",
    "        # encoding\n",
    "        _, (hn, _) = self.encoder(x) # latent representation\n",
    "        hn.transpose_(0, 1) # batch first\n",
    "        # decoding\n",
    "        x_hat, (_, _) = self.decoder(hn.expand(-1, L, -1)) # expand to stratch embedding over length of the input\n",
    "\n",
    "        return (hn.squeeze(1), x_hat.flip(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_dataset = SGCCDataset(path=DATA_PATH, label=0)\n",
    "anomal_dataset = SGCCDataset(path=DATA_PATH, label=1)\n",
    "\n",
    "train_data, val_data, test_normal_data = utils.data.random_split(normal_dataset, [len(normal_dataset) - 2*len(anomal_dataset), len(anomal_dataset), len(anomal_dataset)])\n",
    "test_data = utils.data.ConcatDataset([test_normal_data, anomal_dataset])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = LSTMAE().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "logger = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = len(train_loader)\n",
    "val_len = len(val_loader)\n",
    "train_losses = []\n",
    "train_embed = []\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    # training step\n",
    "    t = tqdm(train_loader)\n",
    "    model.train()\n",
    "    for ii, batch in enumerate(t):\n",
    "        optimizer.zero_grad()\n",
    "        _, x, _ = batch\n",
    "        z, x_hat = model(x.to(device))\n",
    "        loss = loss_fn(x, x_hat)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # logging\n",
    "        train_losses.append(loss.item())\n",
    "        train_embed.append(z.cpu().detach().numpu().squeeze())\n",
    "        t.set_description('Train Loss: {.5%d}'.format(loss.item()))\n",
    "        \n",
    "        step = epoch * train_len + ii\n",
    "        logger.add_scalar('Loss/Train', loss.item(), step)\n",
    "        # TODO add embeddings logging\n",
    "\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    val_embed = []\n",
    "    t = tqdm(val_loader)\n",
    "    for ii, batch in enumerate(t):\n",
    "        with torch.no_grad():\n",
    "            y, x, _ = batch\n",
    "            z, x_hat = model(x.to(device))\n",
    "            loss = loss_fn(x, x_hat)\n",
    "            \n",
    "            val_losses.append(loss.item())\n",
    "            val_embed.append(z.cpu().detach().numpy().squeeze())\n",
    "            t.set_description('Val Loss: {.5%d}'.format(loss.item()))\n",
    "            step = epoch * val_len + ii\n",
    "            logger.add_scalar('Loss/Val', loss.item(), step)\n",
    "            \n",
    "            \n",
    "    # test on normal and anomal \n",
    "\n",
    "    # avarage metrics after each epoch\n",
    "    train_losses = np.array(train_losses)\n",
    "    val_losses = np.array(val_losses)    \n",
    "    train_mean, train_std = train_losses.mean(), train_losses.std()\n",
    "    val_mean, val_std = val_losses.mean(), val_losses.std()\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO adapt this structure to different encoder-decoder architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
