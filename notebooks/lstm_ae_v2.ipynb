{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import utils\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import os \n",
    "from datetime import datetime\n",
    "\n",
    "# import tensorflow as tf\n",
    "# import tensorboard as tb\n",
    "# tf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/ivan.zorin/dev/code/ntl/')\n",
    "\n",
    "from data.data import sgcc_train_test_split, SGCCDataset\n",
    "from models.models import LSTMAE_old\n",
    "from utils.utils import compute_roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs\n",
    "# data_path = '/Users/ivan_zorin/Documents/DEV/data/sgcc/data.csv'\n",
    "\n",
    "data_path = '/home/ivan.zorin/dev/data/sgcc/data.csv'\n",
    "experiment_name = 'lstm_ae'\n",
    "date = datetime.today().strftime('%Y-%m-%d_%H:%M:%S')\n",
    "run_path = os.path.join('/home/ivan.zorin/dev/logs/', experiment_name, date)\n",
    "scale = 'minmax'\n",
    "nan_ratio = 0.7\n",
    "batch_size = 32\n",
    "\n",
    "input_size = 1\n",
    "hidden_size = [64]\n",
    "lr = 0.0001\n",
    "factor = 0.5\n",
    "patience = 3\n",
    "\n",
    "N_epochs = 20\n",
    "val_logging_step = 5\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    assert False, 'cuda is not available'\n",
    "\n",
    "\n",
    "# data\n",
    "normal_dataset = SGCCDataset(path=data_path, label=0, scale=scale, nan_ratio=nan_ratio)\n",
    "anomal_dataset = SGCCDataset(path=data_path, label=1, scale=scale)\n",
    "\n",
    "train_data, val_data, test_normal_data = utils.data.random_split(normal_dataset, [len(normal_dataset) - 2*len(anomal_dataset), len(anomal_dataset), len(anomal_dataset)])\n",
    "test_data = utils.data.ConcatDataset([test_normal_data, anomal_dataset])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# model and train utils\n",
    "model = LSTMAE_old(input_size, hidden_size).to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, mode='min', factor=factor, patience=patience, verbose=True)\n",
    "loss_fn = nn.L1Loss()\n",
    "logger = torch.utils.tensorboard.SummaryWriter(run_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_grad_norm(model, norm_type=2):\n",
    "    name_norm = {}\n",
    "    with torch.no_grad():\n",
    "        for p in model.named_parameters():\n",
    "            if p[1].grad is not None and p[1].requires_grad:\n",
    "                name_norm[p[0]] = torch.norm(p[1], norm_type).item()\n",
    "    \n",
    "    return name_norm\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.train()\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "optim.zero_grad()\n",
    "y, x, _ = batch\n",
    "x = x.to(device)\n",
    "z, x_hat = model(x)\n",
    "loss = loss_fn(x, x_hat)\n",
    "\n",
    "loss.backward()\n",
    "optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoder.0.weight_ih_l0': 1.1585909128189087,\n",
       " 'encoder.0.weight_hh_l0': 9.251689910888672,\n",
       " 'encoder.0.bias_ih_l0': 1.153677225112915,\n",
       " 'encoder.0.bias_hh_l0': 1.1564085483551025,\n",
       " 'decoder.0.weight_ih_l0': 9.124003410339355,\n",
       " 'decoder.0.weight_hh_l0': 1.101063847541809,\n",
       " 'decoder.0.bias_ih_l0': 0.8523733019828796,\n",
       " 'decoder.0.bias_hh_l0': 1.2631181478500366}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norms = inspect_grad_norm(model)\n",
    "norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivan.zorin/.conda/pytorch/lib/python3.11/site-packages/sklearn/utils/_array_api.py:185: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  array = numpy.asarray(array, order=order, dtype=dtype)\n",
      "/home/ivan.zorin/.conda/pytorch/lib/python3.11/site-packages/sklearn/utils/_array_api.py:185: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  array = numpy.asarray(array, order=order, dtype=dtype)\n",
      "/home/ivan.zorin/.conda/pytorch/lib/python3.11/site-packages/sklearn/utils/_array_api.py:185: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  array = numpy.asarray(array, order=order, dtype=dtype)\n",
      "/home/ivan.zorin/.conda/pytorch/lib/python3.11/site-packages/sklearn/utils/_array_api.py:185: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  array = numpy.asarray(array, order=order, dtype=dtype)\n",
      "  0%|          | 0/20 [00:48<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "unknown format is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 63\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m val_logging_step \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     60\u001b[0m     logger\u001b[39m.\u001b[39madd_embedding(tag\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval/embs\u001b[39m\u001b[39m'\u001b[39m, mat\u001b[39m=\u001b[39mval_embeddings, global_step\u001b[39m=\u001b[39mepoch)\n\u001b[0;32m---> 63\u001b[0m _, fig, (FPR, TPR, auc_score) \u001b[39m=\u001b[39m compute_roc_auc(val_losses, val_labels, pyplot\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     64\u001b[0m logger\u001b[39m.\u001b[39madd_scalar(\u001b[39m'\u001b[39m\u001b[39mval/auc-score\u001b[39m\u001b[39m'\u001b[39m, auc_score, epoch)\n\u001b[1;32m     65\u001b[0m logger\u001b[39m.\u001b[39madd_figure(tag\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval/roc-auc\u001b[39m\u001b[39m'\u001b[39m, fig\u001b[39m=\u001b[39mfig, global_step\u001b[39m=\u001b[39mepoch)\n",
      "File \u001b[0;32m/home/ivan.zorin/dev/code/ntl/utils/utils.py:221\u001b[0m, in \u001b[0;36mcompute_roc_auc\u001b[0;34m(scores, labels, pyplot)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_roc_auc\u001b[39m(scores, labels, pyplot\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    219\u001b[0m     \u001b[39m# TODO log roc_curve raw data or scores and labels\u001b[39;00m\n\u001b[0;32m--> 221\u001b[0m     fpr, tpr, thresh \u001b[39m=\u001b[39m roc_curve(labels, scores)\n\u001b[1;32m    222\u001b[0m     auc \u001b[39m=\u001b[39m roc_auc_score(labels, scores)\n\u001b[1;32m    223\u001b[0m     diagonal \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlinspace(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(fpr))\n",
      "File \u001b[0;32m/home/ivan.zorin/.conda/pytorch/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:992\u001b[0m, in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mroc_curve\u001b[39m(\n\u001b[1;32m    905\u001b[0m     y_true, y_score, \u001b[39m*\u001b[39m, pos_label\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, drop_intermediate\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    906\u001b[0m ):\n\u001b[1;32m    907\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute Receiver operating characteristic (ROC).\u001b[39;00m\n\u001b[1;32m    908\u001b[0m \n\u001b[1;32m    909\u001b[0m \u001b[39m    Note: this implementation is restricted to the binary classification task.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    990\u001b[0m \u001b[39m    array([1.8 , 0.8 , 0.4 , 0.35, 0.1 ])\u001b[39;00m\n\u001b[1;32m    991\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 992\u001b[0m     fps, tps, thresholds \u001b[39m=\u001b[39m _binary_clf_curve(\n\u001b[1;32m    993\u001b[0m         y_true, y_score, pos_label\u001b[39m=\u001b[39;49mpos_label, sample_weight\u001b[39m=\u001b[39;49msample_weight\n\u001b[1;32m    994\u001b[0m     )\n\u001b[1;32m    996\u001b[0m     \u001b[39m# Attempt to drop thresholds corresponding to points in between and\u001b[39;00m\n\u001b[1;32m    997\u001b[0m     \u001b[39m# collinear with other points. These are always suboptimal and do not\u001b[39;00m\n\u001b[1;32m    998\u001b[0m     \u001b[39m# appear on a plotted ROC curve (and thus do not affect the AUC).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1003\u001b[0m     \u001b[39m# but does not drop more complicated cases like fps = [1, 3, 7],\u001b[39;00m\n\u001b[1;32m   1004\u001b[0m     \u001b[39m# tps = [1, 2, 4]; there is no harm in keeping too many thresholds.\u001b[39;00m\n\u001b[1;32m   1005\u001b[0m     \u001b[39mif\u001b[39;00m drop_intermediate \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(fps) \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n",
      "File \u001b[0;32m/home/ivan.zorin/.conda/pytorch/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:749\u001b[0m, in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    747\u001b[0m y_type \u001b[39m=\u001b[39m type_of_target(y_true, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my_true\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    748\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (y_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m (y_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m pos_label \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m)):\n\u001b[0;32m--> 749\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m format is not supported\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(y_type))\n\u001b[1;32m    751\u001b[0m check_consistent_length(y_true, y_score, sample_weight)\n\u001b[1;32m    752\u001b[0m y_true \u001b[39m=\u001b[39m column_or_1d(y_true)\n",
      "\u001b[0;31mValueError\u001b[0m: unknown format is not supported"
     ]
    }
   ],
   "source": [
    "# training\n",
    "train_len = len(train_loader)\n",
    "val_len = len(val_loader)\n",
    "\n",
    "for epoch in trange(N_epochs, total=N_epochs):\n",
    "# for epoch in range(N_epochs):\n",
    "    \n",
    "    train_losses, val_losses = [], []\n",
    "    train_embeddings, val_embeddings = [], []\n",
    "    val_labels = []\n",
    "\n",
    "    train_iterator = tqdm(train_loader, leave=False, desc='Train')\n",
    "    val_iterator = tqdm(val_loader, leave=False, desc='Val')\n",
    "    \n",
    "    model.train()\n",
    "    for i, batch in enumerate(train_iterator):\n",
    "        optim.zero_grad()\n",
    "        y, x, _ = batch\n",
    "        x = x.to(device)\n",
    "        z, x_hat = model(x)\n",
    "        loss = loss_fn(x, x_hat)\n",
    "        \n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        train_embeddings.append(z.detach().cpu().numpy().squeeze())\n",
    "        step = i + train_len * epoch\n",
    "        logger.add_scalar('train/loss', loss.item(), step)\n",
    "    \n",
    "    \n",
    "    train_embeddings = np.concatenate(train_embeddings)\n",
    "    train_loss = sum(train_losses) / len(train_losses)\n",
    "    logger.add_embedding(tag='train/embs', mat=train_embeddings, global_step=epoch)\n",
    "    \n",
    "    # inspect_grad_norm(model)\n",
    "    # log gradient norms \n",
    "    # logger.\n",
    "\n",
    "    model.eval()\n",
    "    for i, batch in enumerate(val_iterator):\n",
    "        with torch.no_grad():\n",
    "            y, x, _ = batch\n",
    "            x = x.to(device)\n",
    "            z, x_hat = model(x)\n",
    "            loss = loss_fn(x, x_hat)\n",
    "            \n",
    "            val_labels.append(y)\n",
    "            val_losses.append(loss.item())\n",
    "            val_embeddings.append(z.detach().cpu().numpy().squeeze())\n",
    "            step = i + train_len * epoch\n",
    "            logger.add_scalar('val/loss', loss.item(), step)\n",
    "            \n",
    "    val_loss = sum(val_losses) / len(val_losses)\n",
    "    scheduler.step(val_loss)\n",
    "    logger.add_scalars('loss', {'train': train_loss, 'val': val_loss}, epoch)\n",
    "    \n",
    "    val_embeddings = np.concatenate(val_embeddings)\n",
    "    if epoch % val_logging_step == 0:\n",
    "        logger.add_embedding(tag='val/embs', mat=val_embeddings, global_step=epoch)\n",
    "    \n",
    "    \n",
    "    _, fig, (FPR, TPR, auc_score) = compute_roc_auc(val_losses, val_labels, pyplot=True)\n",
    "    logger.add_scalar('val/auc-score', auc_score, epoch)\n",
    "    logger.add_figure(tag='val/roc-auc', fig=fig, global_step=epoch)\n",
    "    \n",
    "    grad_norms = inspect_grad_norm(model)\n",
    "    logger.add_scalars('grad_norm', grad_norms, epoch)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
