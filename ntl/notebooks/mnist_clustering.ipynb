{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOCAL\n",
    "# DATA_PATH = '/Users/ivan_zorin/Documents/DEV/data/mnist'\n",
    "# LOG_PATH = '/Users/ivan_zorin/Documents/DEV/runs/ntl/mnist'\n",
    "\n",
    "\n",
    "# ZHORES\n",
    "DATA_PATH = '/trinity/home/ivan.zorin/dev/data/'\n",
    "PROJECT_PATH = '/trinity/home/ivan.zorin/dev/code/ntl/'\n",
    "LOG_PATH = '/beegfs/home/ivan.zorin/dev/logs/mnist'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "\n",
    "import torch \n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from torch.optim import Adam\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import sys\n",
    "sys.path.append(PROJECT_PATH)\n",
    "from ntl.utils import get_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data = MNIST(DATA_PATH, train=True, download=True, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select any class to be anomalous data\n",
    "anomal_class = 5\n",
    "anomal_idxs = np.where(mnist_data.targets == anomal_class)[0]\n",
    "# get idxs of anomalous samples\n",
    "normal_idxs = set(np.arange(len(mnist_data))) - set(anomal_idxs)\n",
    "normal_idxs = np.array(list(normal_idxs))\n",
    "# split dataset into: \n",
    "#  - normal_train - normal samples for training\n",
    "#  - anomal_test, normal_test = test_data - both normal and anomalous samples\n",
    "#  - anomal_exposed - anomalous samples seen by a model during train\n",
    "normal_mnist = data.Subset(mnist_data, normal_idxs)\n",
    "anomal_mnist = data.Subset(mnist_data, anomal_idxs) \n",
    "normal_train, normal_test = data.random_split(normal_mnist, [normal_idxs.shape[0] - anomal_idxs.shape[0], anomal_idxs.shape[0]]) # NOTE \n",
    "\n",
    "num_exposed_outliers = 8\n",
    "anomal_test, anomal_exposed = data.random_split(anomal_mnist, [len(anomal_mnist) - num_exposed_outliers, num_exposed_outliers]) # NOTE\n",
    "test_data = data.ConcatDataset([normal_test, anomal_test]) # NOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyModel(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.linear = nn.Linear(emb_dim, emb_dim)\n",
    "        self.layer = nn.Identity()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        bs = x.shape[0]\n",
    "        emb = torch.rand(bs, self.emb_dim).to(x.device)\n",
    "        return emb, x\n",
    "    \n",
    "    \n",
    "    \n",
    "class AEModel(nn.Module):\n",
    "    def __init__(self, bias=True):\n",
    "        super().__init__()\n",
    "        self.bias = bias\n",
    "        channels = [1, 4, 16, 32, 64]\n",
    "        strides = [2, 2, 1, 1]\n",
    "        layers = []\n",
    "        for i in range(len(channels) - 1):\n",
    "\n",
    "            layers += [\n",
    "                nn.Conv2d(in_channels=channels[i], out_channels=channels[i+1], stride=strides[i], kernel_size=3, padding=0, bias=bias),\n",
    "                nn.BatchNorm2d(num_features=channels[i+1]),\n",
    "                nn.Dropout(0.15),\n",
    "                nn.ReLU(),\n",
    "                # nn.MaxPool2d(kernel_size=2, padding=1)\n",
    "            ]\n",
    "            \n",
    "        # layers += [nn.Flatten()]    \n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        \n",
    "        channels.reverse()\n",
    "        strides.reverse()\n",
    "        output_paddings = [0, 0, 0, 1]\n",
    "        layers = []\n",
    "        for i in range(len(channels) - 1):\n",
    "\n",
    "            layers += [\n",
    "                nn.ConvTranspose2d(in_channels=channels[i], out_channels=channels[i+1], stride=strides[i], kernel_size=3, padding=0, bias=bias, output_padding=output_paddings[i]),\n",
    "                nn.BatchNorm2d(num_features=channels[i+1]),\n",
    "                nn.Dropout(0.15),\n",
    "                nn.ReLU()\n",
    "            ]\n",
    "            \n",
    "        self.decoder = nn.Sequential(*layers)\n",
    "            \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "    \n",
    "    def decode(self, emb):\n",
    "        return self.decoder(emb)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        emb = self.encoder(x)\n",
    "        x = self.decoder(emb)\n",
    "        emb = emb.reshape(emb.shape[0], -1)\n",
    "        \n",
    "        return emb, x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################IS NOT USED################################################\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, eps=0.1):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        \n",
    "    def prepare_terms(self, embeddings, negatives):\n",
    "        bs = embeddings.shape[0]\n",
    "        anchors, positives = embeddings[:bs//2], embeddings[bs//2:]\n",
    "        if negatives.shape[0] < bs // 2:\n",
    "            tile_shape = [int(np.ceil((bs // 2) / negatives.shape[0]))] + [1] * (len(negatives.shape) - 1)\n",
    "            negatives = negatives.tile(*tile_shape)[:bs // 2]    \n",
    "            perm_idxs = torch.randperm(negatives.shape[0])\n",
    "            negatives = negatives[perm_idxs]\n",
    "        else:\n",
    "            perm_idxs = torch.randperm(negatives.shape[0])\n",
    "            negatives = negatives[perm_idxs]\n",
    "            negatives = negatives[:bs // 2]\n",
    "            \n",
    "        return anchors, positives, negatives\n",
    "    \n",
    "    def forward(self, embeddings, negatives):\n",
    "        anchors, positives, negatives = self.prepare_terms(embeddings, negatives)\n",
    "    \n",
    "        # compute loss\n",
    "        loss = torch.norm(anchors - positives, 2)\n",
    "#######################################################################\n",
    "\n",
    "\n",
    "def prepare_terms(embeddings, negatives):\n",
    "    \"\"\"\n",
    "    prepare anchor, positive and negative terms of triplet loss\n",
    "    \"\"\"\n",
    "    bs = embeddings.shape[0]\n",
    "    anchors, positives = embeddings[:bs//2], embeddings[bs//2:]\n",
    "    if negatives.shape[0] < bs // 2:\n",
    "        tile_shape = [int(np.ceil((bs // 2) / negatives.shape[0]))] + [1] * (len(negatives.shape) - 1)\n",
    "        negatives = negatives.tile(*tile_shape)[:bs // 2]    \n",
    "        perm_idxs = torch.randperm(negatives.shape[0])\n",
    "        negatives = negatives[perm_idxs]\n",
    "    else:\n",
    "        perm_idxs = torch.randperm(negatives.shape[0])\n",
    "        negatives = negatives[perm_idxs]\n",
    "        negatives = negatives[:bs // 2]\n",
    "        \n",
    "    return anchors, positives, negatives\n",
    "\n",
    "\n",
    "def vis_of_random_sample(model, loader, device):\n",
    "    \"\"\"\n",
    "    returns random sample from the loader and its prediction by the model\n",
    "    \"\"\"\n",
    "    idx = np.random.choice(len(loader.dataset))\n",
    "    x, target = loader.dataset[idx]\n",
    "    _, x_hat = model(x[None].to(device))\n",
    "    x_hat = x_hat.detach().cpu().squeeze(0)\n",
    "    \n",
    "    return x, x_hat\n",
    "\n",
    "\n",
    "def tsne_of_embeddings(embs, targets, names, title=''):\n",
    "    \"\"\"\n",
    "    visualizes the embeddings with tSNE\n",
    "    \"\"\"\n",
    "    sizes = [emb.shape[0] for emb in embs]\n",
    "    new_targets = []\n",
    "    new_targets.append(np.zeros(sizes[0])) # normal\n",
    "    new_targets.append(np.ones(sizes[1])) # anomal\n",
    "    new_targets.append(np.ones(sizes[2])) # exposed\n",
    "    \n",
    "    embs_stacked = np.concatenate(embs)\n",
    "    \n",
    "    embs2d = TSNE(2).fit_transform(embs_stacked)\n",
    "    fig = plt.figure()\n",
    "    plt.scatter(embs2d[:sizes[0], 0], embs2d[:sizes[0], 1], color='b', marker='.', label=names[0], alpha=0.5)\n",
    "    # test normal\n",
    "    plt.scatter(embs2d[sizes[0]:sizes[0] + sizes[1], 0], embs2d[sizes[0]:sizes[0] + sizes[1], 1], color='g', marker='1', label=names[1], alpha=0.5)\n",
    "    # test anomal\n",
    "    plt.scatter(embs2d[-sizes[2]:-1, 0], embs2d[-sizes[2]:-1, 1], color='r', marker='x', label=names[2], alpha=1)\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    # plt.show()\n",
    "        \n",
    "    return fig\n",
    "\n",
    "# fig, embs2d = tsne_of_embeddings(embs, targets, ['normal', 'anomal', 'exposed'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "batch_size = 64\n",
    "lr = 0.001\n",
    "triplet_loss_weight = 0.5\n",
    "is_contrastive = True\n",
    "n_epochs = 50\n",
    "\n",
    "# loaders: train, test_normal, test_anomalous, exposed\n",
    "train_loader = data.DataLoader(normal_train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = data.DataLoader(test_data, batch_size=batch_size)\n",
    "test_normal_loader = data.DataLoader(normal_test, batch_size=batch_size, shuffle=False)\n",
    "test_anomal_loader = data.DataLoader(anomal_test, batch_size=batch_size, shuffle=False)\n",
    "exposed_loader = data.DataLoader(anomal_exposed, batch_size=len(anomal_exposed))\n",
    "\n",
    "# model\n",
    "model = AEModel().to(device)\n",
    "\n",
    "# optimizer\n",
    "optim = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "# loss\n",
    "rec_loss_fn = nn.MSELoss(reduction='none')\n",
    "triplet_loss_fn = nn.TripletMarginLoss(reduction='none')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(logger, epoch):\n",
    "    model.train()\n",
    "    # t = tqdm(train_loader, leave=False, )\n",
    "    # t.set_description('train')\n",
    "    t = train_loader\n",
    "    for i, batch in enumerate(t):\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        x, target = batch\n",
    "        x = x.to(device)\n",
    "        emb, x_hat = model(x)\n",
    "        # LOSS\n",
    "        # reconstruction loss\n",
    "        rec_loss = rec_loss_fn(x, x_hat)\n",
    "        rec_loss = rec_loss.mean(list(range(1, len(rec_loss.shape))))\n",
    "        # contrastive loss\n",
    "        if is_contrastive:\n",
    "            outliers, _ = next(iter(exposed_loader))\n",
    "            # outliers = outliers\n",
    "            neg, _ = model(outliers.to(device))\n",
    "            anchor, pos, neg = prepare_terms(emb, neg)\n",
    "            contrastive_loss = triplet_loss_fn(anchor, pos, neg)\n",
    "        else:\n",
    "            contrastive_loss = 0\n",
    "        # total loss\n",
    "        rec_loss = rec_loss.mean()\n",
    "        contrastive_loss = contrastive_loss.mean()\n",
    "        loss = rec_loss + triplet_loss_weight * contrastive_loss\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        step = epoch * len(t) + i\n",
    "        logger.add_scalar('train/rec_loss', rec_loss.item(), step)\n",
    "        logger.add_scalar('train/contrastive_loss', contrastive_loss.item(), step)\n",
    "        logger.add_scalar('train/loss', loss.item(), step)\n",
    "    \n",
    "    \n",
    "def validate(logger, epoch):\n",
    "    model.eval()\n",
    "    \n",
    "    def validate_on_loader(loader, loader_name=''):\n",
    "        embeddings = []\n",
    "        losses = []\n",
    "        targets = []\n",
    "        # t = tqdm(loader, leave=False)\n",
    "        # t.set_description(loader_name)\n",
    "        t = loader\n",
    "        for i, batch in enumerate(t):\n",
    "            with torch.no_grad():\n",
    "                x, target = batch\n",
    "                x = x.to(device)\n",
    "                emb, x_hat = model(x)\n",
    "                loss = F.mse_loss(x, x_hat, reduction='none')\n",
    "                loss = loss.mean(list(range(1, len(loss.shape)))).detach().cpu().numpy()\n",
    "                \n",
    "                losses.append(loss)\n",
    "                embeddings.append(emb.detach().cpu().numpy())\n",
    "                targets.append(target.cpu().numpy())\n",
    "\n",
    "        losses = np.concatenate(losses)\n",
    "        embeddings = np.concatenate(embeddings)\n",
    "        targets = np.concatenate(targets)\n",
    "                \n",
    "        return losses, embeddings, targets\n",
    "    \n",
    "    embeddings = []\n",
    "    losses = []\n",
    "    \n",
    "    normal_loss, normal_emb, normal_targets = validate_on_loader(test_normal_loader, 'normal')\n",
    "    anomal_loss, anomal_emb, anomal_targets = validate_on_loader(test_anomal_loader, 'anomalous')\n",
    "    exposed_loss, exposed_emb, exposed_targets = validate_on_loader(exposed_loader, 'exposed')\n",
    "    \n",
    "    # log embeddings\n",
    "    embeddings = np.concatenate([normal_emb, anomal_emb, exposed_emb])\n",
    "    # new_targets = np.concatenate([np.zeros[normal_emb.shape[0], np.ones(anomal_emb.shape[0])], 2 * np.ones(exposed_emb.shape[0])])\n",
    "    labels = ['normal'] * normal_emb.shape[0] + ['anomal'] * anomal_emb.shape[0] + ['exposed'] * exposed_emb.shape[0]\n",
    "    \n",
    "    logger.add_embedding(embeddings, labels, global_step=epoch)\n",
    "    \n",
    "    # plot embeddings, visualizing slices\n",
    "    fig = tsne_of_embeddings([normal_emb, anomal_emb, exposed_emb], [normal_targets, anomal_targets, exposed_targets], names=['normal', 'anomal', 'exposed'])\n",
    "    logger.add_figure('embeddings/tsne', fig, epoch)\n",
    "    \n",
    "    \n",
    "    # visualization of reconstruction of random NORMAL sample\n",
    "    x, x_hat = vis_of_random_sample(model, test_normal_loader, device)\n",
    "    logger.add_image('normal/origin', x, epoch)\n",
    "    logger.add_image('normal/reconstructed', x_hat, epoch)\n",
    "    \n",
    "    # EXPOSED sample\n",
    "    x, x_hat = vis_of_random_sample(model, exposed_loader, device)\n",
    "    logger.add_image('exposed/origin', x, epoch)\n",
    "    logger.add_image('exposed/reconstructed', x_hat, epoch)\n",
    "    \n",
    "    # return [normal_emb, anomal_emb, exposed_emb], [normal_targets, anomal_targets, exposed_targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run is at /beegfs/home/ivan.zorin/dev/logs/mnist/23_10_23_15_19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9a73b8b610341eb8d1ad19cfb1c8643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_dir = os.path.join(LOG_PATH, get_date())\n",
    "print(f'run is at {_dir}')\n",
    "logger = SummaryWriter(_dir)\n",
    "n_epochs = 50\n",
    "t = tqdm(range(5))\n",
    "for epoch in t:\n",
    "    t.set_description(f'epoch {epoch}')\n",
    "    train(logger, epoch)\n",
    "    validate(logger, epoch)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
