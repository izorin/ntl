{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch \n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from torch.optim import Adam\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import sys\n",
    "sys.path.append('/Users/ivan_zorin/Documents/DEV/code/ntl/')\n",
    "from utils import get_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/Users/ivan_zorin/Documents/DEV/data/mnist'\n",
    "\n",
    "mnist_data = MNIST(DATA_PATH, train=True, download=True, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomal_class = 5\n",
    "anomal_idxs = np.where(mnist_data.targets == anomal_class)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_idxs = set(np.arange(len(mnist_data))) - set(anomal_idxs)\n",
    "normal_idxs = np.array(list(normal_idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_mnist = data.Subset(mnist_data, normal_idxs)\n",
    "anomal_mnist = data.Subset(mnist_data, anomal_idxs)\n",
    "normal_train, normal_test = data.random_split(normal_mnist, [normal_idxs.shape[0] - anomal_idxs.shape[0], anomal_idxs.shape[0]])\n",
    "num_exposed_outliers = 8\n",
    "anomal_test, anomal_exposed = data.random_split(anomal_mnist, [len(anomal_mnist) - num_exposed_outliers, num_exposed_outliers])\n",
    "test_data = data.ConcatDataset([normal_test, anomal_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.linear = nn.Linear(emb_dim, emb_dim)\n",
    "        self.layer = nn.Identity()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        bs = x.shape[0]\n",
    "        emb = torch.rand(bs, self.emb_dim).to(x.device)\n",
    "        return emb, x\n",
    "    \n",
    "class AEModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = []\n",
    "        \n",
    "        \n",
    "    def forward(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AEModel(nn.Module):\n",
    "    def __init__(self, bias=True):\n",
    "        super().__init__()\n",
    "        self.bias = bias\n",
    "        channels = [1, 4, 16, 32, 64]\n",
    "        strides = [2, 2, 1, 1]\n",
    "        layers = []\n",
    "        for i in range(len(channels) - 1):\n",
    "\n",
    "            layers += [\n",
    "                nn.Conv2d(in_channels=channels[i], out_channels=channels[i+1], stride=strides[i], kernel_size=3, padding=0, bias=bias),\n",
    "                nn.BatchNorm2d(num_features=channels[i+1]),\n",
    "                nn.Dropout(0.15),\n",
    "                nn.ReLU(),\n",
    "                # nn.MaxPool2d(kernel_size=2, padding=1)\n",
    "            ]\n",
    "            \n",
    "        # layers += [nn.Flatten()]    \n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        \n",
    "        channels.reverse()\n",
    "        strides.reverse()\n",
    "        output_paddings = [0, 0, 0, 1]\n",
    "        layers = []\n",
    "        for i in range(len(channels) - 1):\n",
    "\n",
    "            layers += [\n",
    "                nn.ConvTranspose2d(in_channels=channels[i], out_channels=channels[i+1], stride=strides[i], kernel_size=3, padding=0, bias=bias, output_padding=output_paddings[i]),\n",
    "                nn.BatchNorm2d(num_features=channels[i+1]),\n",
    "                nn.Dropout(0.15),\n",
    "                nn.ReLU()\n",
    "            ]\n",
    "            \n",
    "        self.decoder = nn.Sequential(*layers)\n",
    "            \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "    \n",
    "    def decode(self, emb):\n",
    "        return self.decoder(emb)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        emb = self.encoder(x)\n",
    "        x = self.decoder(emb)\n",
    "        emb = emb.reshape(emb.shape[0], -1)\n",
    "        \n",
    "        return emb, x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, eps=0.1):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        \n",
    "    def prepare_terms(self, embeddings, negatives):\n",
    "        bs = embeddings.shape[0]\n",
    "        anchors, positives = embeddings[:bs//2], embeddings[bs//2:]\n",
    "        if negatives.shape[0] < bs // 2:\n",
    "            tile_shape = [int(np.ceil((bs // 2) / negatives.shape[0]))] + [1] * (len(negatives.shape) - 1)\n",
    "            negatives = negatives.tile(*tile_shape)[:bs // 2]    \n",
    "            perm_idxs = torch.randperm(negatives.shape[0])\n",
    "            negatives = negatives[perm_idxs]\n",
    "        else:\n",
    "            perm_idxs = torch.randperm(negatives.shape[0])\n",
    "            negatives = negatives[perm_idxs]\n",
    "            negatives = negatives[:bs // 2]\n",
    "            \n",
    "        return anchors, positives, negatives\n",
    "    \n",
    "    def forward(self, embeddings, negatives):\n",
    "        anchors, positives, negatives = self.prepare_terms(embeddings, negatives)\n",
    "    \n",
    "        # compute loss\n",
    "        loss = torch.norm(anchors - positives, 2)\n",
    "        \n",
    "\n",
    "\n",
    "def prepare_terms(embeddings, negatives):\n",
    "    bs = embeddings.shape[0]\n",
    "    anchors, positives = embeddings[:bs//2], embeddings[bs//2:]\n",
    "    if negatives.shape[0] < bs // 2:\n",
    "        tile_shape = [int(np.ceil((bs // 2) / negatives.shape[0]))] + [1] * (len(negatives.shape) - 1)\n",
    "        negatives = negatives.tile(*tile_shape)[:bs // 2]    \n",
    "        perm_idxs = torch.randperm(negatives.shape[0])\n",
    "        negatives = negatives[perm_idxs]\n",
    "    else:\n",
    "        perm_idxs = torch.randperm(negatives.shape[0])\n",
    "        negatives = negatives[perm_idxs]\n",
    "        negatives = negatives[:bs // 2]\n",
    "        \n",
    "    return anchors, positives, negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "batch_size = 64\n",
    "lr = 0.001\n",
    "triplet_loss_weight = 0.5\n",
    "is_contrastive = True\n",
    "log_path = '/Users/ivan_zorin/Documents/DEV/runs/ntl/mnist'\n",
    "n_epochs = 20\n",
    "\n",
    "# loaders\n",
    "train_loader = data.DataLoader(normal_train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = data.DataLoader(test_data, batch_size=batch_size)\n",
    "test_normal_loader = data.DataLoader(normal_test, batch_size=bs, shuffle=False)\n",
    "test_anomal_loader = data.DataLoader(anomal_test, batch_size=bs, shuffle=False)\n",
    "exposed_loader = data.DataLoader(anomal_exposed, batch_size=len(anomal_exposed))\n",
    "\n",
    "\n",
    "\n",
    "# model\n",
    "model = AEModel().to(device)\n",
    "\n",
    "# optimizer\n",
    "optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# loss\n",
    "\n",
    "rec_loss_fn = nn.MSELoss(reduction='none')\n",
    "triplet_loss_fn = nn.TripletMarginLoss(reduction='none')\n",
    "\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_of_embeddings():\n",
    "    pass\n",
    "\n",
    "\n",
    "def vis_of_random_sample(loader):\n",
    "    idx = np.random.choice(len(loader.dataset))\n",
    "    x, target = loader.dataset[idx]\n",
    "    _, x_hat = model(x[None].to(device))\n",
    "    x_hat = x_hat.detach().cpu().squeeze(0)\n",
    "    \n",
    "    return x, x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(logger, epoch):\n",
    "    model.train()\n",
    "    t = train_loader\n",
    "    for i, batch in enumerate(t):\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        x, target = batch\n",
    "        x = x.to(device)\n",
    "        emb, x_hat = model(x)\n",
    "        # LOSS\n",
    "        # reconstruction loss\n",
    "        rec_loss = rec_loss_fn(x, x_hat)\n",
    "        rec_loss = rec_loss.mean(list(range(1, len(rec_loss.shape))))\n",
    "        # contrastive loss\n",
    "        if is_contrastive:\n",
    "            outliers, _ = next(iter(exposed_loader))\n",
    "            # outliers = outliers\n",
    "            neg, _ = model(outliers.to(device))\n",
    "            anchor, pos, neg = prepare_terms(emb, neg)\n",
    "            contrastive_loss = triplet_loss_fn(anchor, pos, neg)\n",
    "        else:\n",
    "            contrastive_loss = 0\n",
    "        # total loss\n",
    "        loss = rec_loss.mean() + triplet_loss_weight * contrastive_loss.mean()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        step = epoch * len(t) + i\n",
    "        logger.add_scalar('train/rec_loss', rec_loss.item(), step)\n",
    "        logger.add_scalar('train/contrastive_loss', contrastive_loss.item(), step)\n",
    "        logger.add_scalar('train/loss', loss.item(), step)\n",
    "    \n",
    "    \n",
    "def validate(logger, epoch):\n",
    "    model.eval()\n",
    "    \n",
    "    random_x = np.random.choice(test_normal_loader.dataset)\n",
    "    \n",
    "    def validate_on_loader(loader):\n",
    "        embeddings = []\n",
    "        losses = []\n",
    "        targets = []\n",
    "        for i, batch in enumerate(loader):\n",
    "            with torch.no_grad():\n",
    "                x, target = batch\n",
    "                x = x.to(device)\n",
    "                emb, x_hat = model(x)\n",
    "                loss = F.mse_loss(x, x_hat, reduction='none')\n",
    "                loss = loss.mean(list(range(1, len(loss.shape)))).detach().cpu().numpy()\n",
    "                \n",
    "                losses.append(loss)\n",
    "                embeddings.append(emb.detach().cpu().numpy())\n",
    "                targets.append(target.cpu().numpy())\n",
    "\n",
    "        losses = np.concatenate(losses)\n",
    "        embeddings = np.concatenate(embeddings)\n",
    "        targets = np.concatenate(targets)\n",
    "                \n",
    "        return losses, embeddings, targets\n",
    "    \n",
    "    embeddings = []\n",
    "    losses = []\n",
    "    \n",
    "    normal_loss, normal_emb, normal_targets = validate_on_loader(test_normal_loader)\n",
    "    anomal_loss, anomal_emb, anomal_targets = validate_on_loader(test_anomal_loader)\n",
    "    exposed_loss, exposed_emb, exposed_targets = validate_on_loader(exposed_loader)\n",
    "    \n",
    "    # stack embeddings\n",
    "    # stack targets\n",
    "    \n",
    "    # plot embeddings, visualizing slices\n",
    "    tsne_of_embeddings()\n",
    "    \n",
    "    # visualization of reconstruction of random NORMAL sample\n",
    "    x, x_hat = vis_of_random_sample(test_normal_loader)\n",
    "    logger.add_image('normal/origin', x, epoch)\n",
    "    logger.add_image('normal/reconstructed', x_hat, epoch)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = SummaryWriter(log_path + '/' + get_date())\n",
    "for epoch in range(1):\n",
    "    train(logger, epoch)\n",
    "    # validate(logger, epoch)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
