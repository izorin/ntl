{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from numpy import ndarray\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk \n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler, robust_scale\n",
    "from sktime.transformations.series.impute import Imputer\n",
    "\n",
    "from functools import partial\n",
    "from types import SimpleNamespace\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# LOCAL\n",
    "# PROJECT_PATH = '/Users/ivan_zorin/Documents/DEV/code/ntl/'\n",
    "# DATA_PATH = '/Users/ivan_zorin/Documents/DEV/data/sgcc/data.csv'\n",
    "# LOG_DIR = '/Users/ivan_zorin/Documents/DEV/runs/debug/trainer'\n",
    "\n",
    "# ZHORES\n",
    "PROJECT_PATH = '/trinity/home/ivan.zorin/dev/code/ntl/'\n",
    "DATA_PATH = '/trinity/home/ivan.zorin/dev/data/sgcc/data.csv'\n",
    "LOG_DIR = '/trinity/home/ivan.zorin/dev/logs/debug/'\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(PROJECT_PATH)\n",
    "from ntl.data import SGCCDataset, data_train_test_split\n",
    "from ntl.data import FillNA, Scale, Reshape, ToTensor, Cutout\n",
    "from ntl.models import AE2dCNN\n",
    "from ntl.trainer import ArgsTrainer\n",
    "from ntl.utils import fix_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 42\n"
     ]
    }
   ],
   "source": [
    "fix_seed(42)\n",
    "\n",
    "transforms = [FillNA('drift'), \n",
    "            Cutout(256), \n",
    "            Scale('robust'), \n",
    "            Reshape((16, 16)),\n",
    "            lambda x: x[None],\n",
    "            ToTensor()\n",
    "]\n",
    "normal_data = SGCCDataset(DATA_PATH, label=0, nan_ratio=0.75, transforms=transforms, year=2016)\n",
    "anomal_data = SGCCDataset(DATA_PATH, label=1, nan_ratio=1.0, transforms=transforms, year=2016)\n",
    "\n",
    "train, test = data_train_test_split(normal_data, anomal_data)\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=1, drop_last=True, shuffle=True)\n",
    "test_loader = DataLoader(test, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = AE2dCNN()\n",
    "loss = nn.MSELoss(reduction='none')\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, factor=0.5, patience=2)\n",
    "logger = SummaryWriter(log_dir=LOG_DIR) \n",
    "\n",
    "config = SimpleNamespace(**{\n",
    "    'debug': True,\n",
    "    'n_debug_batches': np.nan,\n",
    "    'log_step': 5,\n",
    "    'n_epochs': 100\n",
    "})\n",
    "\n",
    "trainer = ArgsTrainer(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=test_loader,\n",
    "    model=model,\n",
    "    loss=loss,\n",
    "    optim=optim,\n",
    "    scheduler=scheduler,\n",
    "    config=config,\n",
    "    logger=logger\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9857668783b8498fa69ac46b124df826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8780 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/trinity/home/ivan.zorin/dev/code/ntl/ntl/notebooks/ae_od.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bzhores.compute/trinity/home/ivan.zorin/dev/code/ntl/ntl/notebooks/ae_od.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/dev/code/ntl/ntl/trainer.py:127\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m train_losses, val_losses \u001b[39m=\u001b[39m [], []\n\u001b[1;32m    126\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mn_epochs):\n\u001b[0;32m--> 127\u001b[0m     train_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_step(epoch)\n\u001b[1;32m    128\u001b[0m     \u001b[39m# train_loss = -1\u001b[39;00m\n\u001b[1;32m    129\u001b[0m     val_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_step(epoch)\n",
      "File \u001b[0;32m~/dev/code/ntl/ntl/trainer.py:103\u001b[0m, in \u001b[0;36mBaseTrainer.train_step\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_step\u001b[39m(\u001b[39mself\u001b[39m, epoch):\n\u001b[1;32m    102\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m--> 103\u001b[0m     losses, _, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshared_step(epoch, step_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    104\u001b[0m     \u001b[39mreturn\u001b[39;00m losses\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/dev/code/ntl/ntl/trainer.py:97\u001b[0m, in \u001b[0;36mBaseTrainer.shared_step\u001b[0;34m(self, epoch, step_name)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39m# except: \u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[39m#     print('error in con')\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mlog_step \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 97\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlogger\u001b[39m.\u001b[39;49madd_embedding(tag\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mstep_name\u001b[39m}\u001b[39;49;00m\u001b[39m/embs\u001b[39;49m\u001b[39m'\u001b[39;49m, mat\u001b[39m=\u001b[39;49membeddings_stash, metadata\u001b[39m=\u001b[39;49mlabels, global_step\u001b[39m=\u001b[39;49mepoch)\n\u001b[1;32m     99\u001b[0m \u001b[39mreturn\u001b[39;00m losses, embeddings_stash, labels\n",
      "File \u001b[0;32m/home/ivan.zorin/.conda/pytorch/lib/python3.11/site-packages/torch/utils/tensorboard/writer.py:954\u001b[0m, in \u001b[0;36mSummaryWriter.add_embedding\u001b[0;34m(self, mat, metadata, label_img, global_step, tag, metadata_header)\u001b[0m\n\u001b[1;32m    949\u001b[0m     make_sprite(label_img, save_path)\n\u001b[1;32m    951\u001b[0m \u001b[39massert\u001b[39;00m (\n\u001b[1;32m    952\u001b[0m     mat\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m    953\u001b[0m ), \u001b[39m\"\u001b[39m\u001b[39mmat should be 2D, where mat.size(0) is the number of data points\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 954\u001b[0m make_mat(mat, save_path)\n\u001b[1;32m    956\u001b[0m \u001b[39m# Filesystem doesn't necessarily have append semantics, so we store an\u001b[39;00m\n\u001b[1;32m    957\u001b[0m \u001b[39m# internal buffer to append to and re-write whole file after each\u001b[39;00m\n\u001b[1;32m    958\u001b[0m \u001b[39m# embedding is added\u001b[39;00m\n\u001b[1;32m    959\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_projector_config\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m/home/ivan.zorin/.conda/pytorch/lib/python3.11/site-packages/torch/utils/tensorboard/_embedding.py:84\u001b[0m, in \u001b[0;36mmake_mat\u001b[0;34m(matlist, save_path)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39mGFile(_gfile_join(save_path, \u001b[39m\"\u001b[39m\u001b[39mtensors.tsv\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     83\u001b[0m     \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m matlist:\n\u001b[0;32m---> 84\u001b[0m         x \u001b[39m=\u001b[39m [\u001b[39mstr\u001b[39;49m(i\u001b[39m.\u001b[39;49mitem()) \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m x]\n\u001b[1;32m     85\u001b[0m         f\u001b[39m.\u001b[39mwrite(tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mas_bytes(\u001b[39m\"\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(x) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m))\n",
      "File \u001b[0;32m/home/ivan.zorin/.conda/pytorch/lib/python3.11/site-packages/torch/utils/tensorboard/_embedding.py:84\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39mGFile(_gfile_join(save_path, \u001b[39m\"\u001b[39m\u001b[39mtensors.tsv\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     83\u001b[0m     \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m matlist:\n\u001b[0;32m---> 84\u001b[0m         x \u001b[39m=\u001b[39m [\u001b[39mstr\u001b[39m(i\u001b[39m.\u001b[39;49mitem()) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m x]\n\u001b[1;32m     85\u001b[0m         f\u001b[39m.\u001b[39mwrite(tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mas_bytes(\u001b[39m\"\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(x) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 1024), (1024,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = [np.random.rand(4, 1024), np.random.rand(1024)]\n",
    "emb[0].shape, emb[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb[1][...].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/trinity/home/ivan.zorin/dev/code/ntl/ntl/notebooks/ae_od.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bzhores.compute/trinity/home/ivan.zorin/dev/code/ntl/ntl/notebooks/ae_od.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m emb2 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mconcatenate(emb)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)"
     ]
    }
   ],
   "source": [
    "emb2 = np.concatenate(emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* normalize data to 0,1 scale | check\n",
    "* fill nans | check\n",
    "* cut-out some piece | check\n",
    "* reshape into matrix (7x52, square, other?) | check\n",
    "* use this matrix as input into AE | check\n",
    "\n",
    "\n",
    "Two ways of transformation \n",
    "The first one \n",
    "1. cut-out some piece\n",
    "2. fill na\n",
    "3. scale \n",
    "4. reshape\n",
    "\n",
    "The second one \n",
    "1. fill na\n",
    "2. scale\n",
    "3. cut-out some piece\n",
    "5. reshape\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
