{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'pytorch (Python 3.11.3)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "mutable default <class 'fairseq.dataclass.configs.CommonConfig'> for field common is not allowed: use default_factory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m \n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset, DataLoader, random_split, ConcatDataset\n\u001b[0;32m----> 8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mfairseq\u001b[39;00m \n\u001b[1;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msequitur\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mseq\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msequitur\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m LSTM_AE\n",
      "File \u001b[0;32m/home/ivan.zorin/.conda/pytorch/lib/python3.11/site-packages/fairseq/__init__.py:20\u001b[0m\n\u001b[1;32m     17\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mpdb\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     19\u001b[0m \u001b[39m# backwards compatibility to support `from fairseq.X import Y`\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfairseq\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistributed\u001b[39;00m \u001b[39mimport\u001b[39;00m utils \u001b[39mas\u001b[39;00m distributed_utils\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfairseq\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlogging\u001b[39;00m \u001b[39mimport\u001b[39;00m meters, metrics, progress_bar  \u001b[39m# noqa\u001b[39;00m\n\u001b[1;32m     23\u001b[0m sys\u001b[39m.\u001b[39mmodules[\u001b[39m\"\u001b[39m\u001b[39mfairseq.distributed_utils\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m distributed_utils\n",
      "File \u001b[0;32m/home/ivan.zorin/.conda/pytorch/lib/python3.11/site-packages/fairseq/distributed/__init__.py:7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright (c) Facebook, Inc. and its affiliates.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# This source code is licensed under the MIT license found in the\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdistributed_timeout_wrapper\u001b[39;00m \u001b[39mimport\u001b[39;00m DistributedTimeoutWrapper\n\u001b[0;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfully_sharded_data_parallel\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     fsdp_enable_wrap,\n\u001b[1;32m      9\u001b[0m     fsdp_wrap,\n\u001b[1;32m     10\u001b[0m     FullyShardedDataParallel,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mlegacy_distributed_data_parallel\u001b[39;00m \u001b[39mimport\u001b[39;00m LegacyDistributedDataParallel\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodule_proxy_wrapper\u001b[39;00m \u001b[39mimport\u001b[39;00m ModuleProxyWrapper\n",
      "File \u001b[0;32m/home/ivan.zorin/.conda/pytorch/lib/python3.11/site-packages/fairseq/distributed/fully_sharded_data_parallel.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Optional\n\u001b[1;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfairseq\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataclass\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconfigs\u001b[39;00m \u001b[39mimport\u001b[39;00m DistributedTrainingConfig\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfairseq\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistributed\u001b[39;00m \u001b[39mimport\u001b[39;00m utils \u001b[39mas\u001b[39;00m dist_utils\n\u001b[1;32m     14\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/home/ivan.zorin/.conda/pytorch/lib/python3.11/site-packages/fairseq/dataclass/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright (c) Facebook, Inc. and its affiliates.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# This source code is licensed under the MIT license found in the\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mconfigs\u001b[39;00m \u001b[39mimport\u001b[39;00m FairseqDataclass\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mconstants\u001b[39;00m \u001b[39mimport\u001b[39;00m ChoiceEnum\n\u001b[1;32m     10\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[1;32m     11\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mFairseqDataclass\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mChoiceEnum\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m ]\n",
      "File \u001b[0;32m/home/ivan.zorin/.conda/pytorch/lib/python3.11/site-packages/fairseq/dataclass/configs.py:1104\u001b[0m\n\u001b[1;32m   1095\u001b[0m     ema_update_freq: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m field(\n\u001b[1;32m   1096\u001b[0m         default\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, metadata\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mhelp\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDo EMA update every this many model updates\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m   1097\u001b[0m     )\n\u001b[1;32m   1098\u001b[0m     ema_fp32: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m field(\n\u001b[1;32m   1099\u001b[0m         default\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   1100\u001b[0m         metadata\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mhelp\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mIf true, store EMA model in fp32 even if model is in fp16\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m   1101\u001b[0m     )\n\u001b[0;32m-> 1104\u001b[0m \u001b[39m@dataclass\u001b[39;49m\n\u001b[1;32m   1105\u001b[0m \u001b[39mclass\u001b[39;49;00m \u001b[39mFairseqConfig\u001b[39;49;00m(FairseqDataclass):\n\u001b[1;32m   1106\u001b[0m     common: CommonConfig \u001b[39m=\u001b[39;49m CommonConfig()\n\u001b[1;32m   1107\u001b[0m     common_eval: CommonEvalConfig \u001b[39m=\u001b[39;49m CommonEvalConfig()\n",
      "File \u001b[0;32m/home/ivan.zorin/.conda/pytorch/lib/python3.11/dataclasses.py:1223\u001b[0m, in \u001b[0;36mdataclass\u001b[0;34m(cls, init, repr, eq, order, unsafe_hash, frozen, match_args, kw_only, slots, weakref_slot)\u001b[0m\n\u001b[1;32m   1220\u001b[0m     \u001b[39mreturn\u001b[39;00m wrap\n\u001b[1;32m   1222\u001b[0m \u001b[39m# We're called as @dataclass without parens.\u001b[39;00m\n\u001b[0;32m-> 1223\u001b[0m \u001b[39mreturn\u001b[39;00m wrap(\u001b[39mcls\u001b[39;49m)\n",
      "File \u001b[0;32m/home/ivan.zorin/.conda/pytorch/lib/python3.11/dataclasses.py:1213\u001b[0m, in \u001b[0;36mdataclass.<locals>.wrap\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m   1212\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap\u001b[39m(\u001b[39mcls\u001b[39m):\n\u001b[0;32m-> 1213\u001b[0m     \u001b[39mreturn\u001b[39;00m _process_class(\u001b[39mcls\u001b[39;49m, init, \u001b[39mrepr\u001b[39;49m, eq, order, unsafe_hash,\n\u001b[1;32m   1214\u001b[0m                           frozen, match_args, kw_only, slots,\n\u001b[1;32m   1215\u001b[0m                           weakref_slot)\n",
      "File \u001b[0;32m/home/ivan.zorin/.conda/pytorch/lib/python3.11/dataclasses.py:958\u001b[0m, in \u001b[0;36m_process_class\u001b[0;34m(cls, init, repr, eq, order, unsafe_hash, frozen, match_args, kw_only, slots, weakref_slot)\u001b[0m\n\u001b[1;32m    955\u001b[0m         kw_only \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    956\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    957\u001b[0m         \u001b[39m# Otherwise it's a field of some type.\u001b[39;00m\n\u001b[0;32m--> 958\u001b[0m         cls_fields\u001b[39m.\u001b[39mappend(_get_field(\u001b[39mcls\u001b[39;49m, name, \u001b[39mtype\u001b[39;49m, kw_only))\n\u001b[1;32m    960\u001b[0m \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m cls_fields:\n\u001b[1;32m    961\u001b[0m     fields[f\u001b[39m.\u001b[39mname] \u001b[39m=\u001b[39m f\n",
      "File \u001b[0;32m/home/ivan.zorin/.conda/pytorch/lib/python3.11/dataclasses.py:815\u001b[0m, in \u001b[0;36m_get_field\u001b[0;34m(cls, a_name, a_type, default_kw_only)\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[39m# For real fields, disallow mutable defaults.  Use unhashable as a proxy\u001b[39;00m\n\u001b[1;32m    812\u001b[0m \u001b[39m# indicator for mutability.  Read the __hash__ attribute from the class,\u001b[39;00m\n\u001b[1;32m    813\u001b[0m \u001b[39m# not the instance.\u001b[39;00m\n\u001b[1;32m    814\u001b[0m \u001b[39mif\u001b[39;00m f\u001b[39m.\u001b[39m_field_type \u001b[39mis\u001b[39;00m _FIELD \u001b[39mand\u001b[39;00m f\u001b[39m.\u001b[39mdefault\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__hash__\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 815\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmutable default \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(f\u001b[39m.\u001b[39mdefault)\u001b[39m}\u001b[39;00m\u001b[39m for field \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    816\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mf\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m is not allowed: use default_factory\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    818\u001b[0m \u001b[39mreturn\u001b[39;00m f\n",
      "\u001b[0;31mValueError\u001b[0m: mutable default <class 'fairseq.dataclass.configs.CommonConfig'> for field common is not allowed: use default_factory"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "from torch import nn\n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data import Dataset, DataLoader, random_split, ConcatDataset\n",
    "\n",
    "import fairseq \n",
    "import sequitur as seq\n",
    "from sequitur.models import LSTM_AE\n",
    "\n",
    "\n",
    "import os \n",
    "import sys\n",
    "sys.path.append('/Users/ivan_zorin/Documents/DEV/code/ntl/')\n",
    "\n",
    "from data.data import SGCCDataset\n",
    "import models\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28778 3612\n"
     ]
    }
   ],
   "source": [
    "data_path = '/Users/ivan_zorin/Documents/DEV/data/sgcc/data.csv'\n",
    "\n",
    "normal_data = SGCCDataset(data_path, label=0, nan_ratio=0.5)\n",
    "anomal_data = SGCCDataset(data_path, label=1, nan_ratio=1.0)\n",
    "\n",
    "print(len(normal_data), len(anomal_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preparation for sequitur\n",
    "train_data = [torch.tensor(sample[1]) for sample in normal_data]\n",
    "test_data = [torch.tensor(sample[1]) for sample in anomal_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1034, 1)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM_AE(\n",
       "  (encoder): Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): LSTM(1, 512, batch_first=True)\n",
       "      (1): LSTM(512, 512, batch_first=True)\n",
       "      (2): LSTM(512, 256, batch_first=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): LSTM(256, 512, batch_first=True)\n",
       "      (1-2): 2 x LSTM(512, 512, batch_first=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_kwargs = {\n",
    "    'input_dim': 1,\n",
    "    'encoding_dim': 256,\n",
    "    'h_dims': [512, 512],\n",
    "    'h_activ': F.relu,\n",
    "    'out_activ': F.relu\n",
    "}\n",
    "\n",
    "# encoding_dim = 256\n",
    "\n",
    "model = LSTM_AE(**model_kwargs)\n",
    "model\n",
    "\n",
    "\n",
    "x = train_data[0]\n",
    "out = model(x)\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_kwargs = {\n",
    "    'input_dim': 1,\n",
    "    'encoding_dim': 256,\n",
    "    'h_dims': [512, 512],\n",
    "    'h_activ': 'relu',\n",
    "    'out_activ': 'relu'\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM_AE(\n",
       "  (encoder): Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): LSTM(1, 512, batch_first=True)\n",
       "      (1): LSTM(512, 512, batch_first=True)\n",
       "      (2): LSTM(512, 256, batch_first=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): LSTM(256, 512, batch_first=True)\n",
       "      (1-2): 2 x LSTM(512, 512, batch_first=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = SequiturLSTMAE(model2_kwargs)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256]), torch.Size([1034, 1]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].shape, out[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder, decoder, embs, losses = seq.quick_train(LSTM_AE, train_data, encoding_dim=encoding_dim, verbose=True, lr=1e-05, epochs=100, denoise=True, kwargs=model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_config = '/Users/ivan_zorin/Documents/DEV/code/ntl/configs/trainer_debug.yaml'\n",
    "path_config = '/Users/ivan_zorin/Documents/DEV/code/ntl/configs/local_pathes.yaml'\n",
    "\n",
    "config = load_config(experiment_config, path_config)\n",
    "\n",
    "device = config.device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_dataset = SGCCDataset(path=config.data_path, label=0, scale=config.scale, nan_ratio=config.nan_ratio)\n",
    "anomal_dataset = SGCCDataset(path=config.data_path, label=1, scale=config.scale)\n",
    "\n",
    "train_data, val_data, test_normal_data = random_split(normal_dataset, [len(normal_dataset) - 2*len(anomal_dataset), len(anomal_dataset), len(anomal_dataset)])\n",
    "test_data = ConcatDataset([test_normal_data, anomal_dataset])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=config.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(test_data, batch_size=config.batch_size, shuffle=config.debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "y, x, _ = batch\n",
    "x = x.to(device)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequiturLSTMAE(\n",
       "  (encoder): Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): LSTM(1, 512, batch_first=True)\n",
       "      (1): LSTM(512, 512, batch_first=True)\n",
       "      (2): LSTM(512, 256, batch_first=True)\n",
       "    )\n",
       "    (h_activ): ReLU()\n",
       "    (out_activ): ReLU()\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): LSTM(256, 512, batch_first=True)\n",
       "      (1-2): 2 x LSTM(512, 512, batch_first=True)\n",
       "    )\n",
       "    (h_activ): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SequiturLSTMAE(LSTM_AE):\n",
    "    def __init__(self, input_dim, encoding_dim, h_dims=[], h_activ=nn.Sigmoid(), out_activ=nn.Tanh()):\n",
    "\n",
    "        h_activ = getattr(nn, h_activ)()\n",
    "        out_activ = getattr(nn, out_activ)()\n",
    "        \n",
    "        super().__init__(input_dim, encoding_dim, h_dims, h_activ, out_activ)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        seq_len = x.shape[1]\n",
    "        print(x.shape)\n",
    "        z = self.encoder(x)\n",
    "        x = self.decoder(z, seq_len)\n",
    "        \n",
    "        return z, x\n",
    "\n",
    "\n",
    "\n",
    "model = SequiturLSTMAE(**config.model_kwargs).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (layers): ModuleList(\n",
       "    (0): LSTM(1, 512, batch_first=True)\n",
       "    (1): LSTM(512, 512, batch_first=True)\n",
       "    (2): LSTM(512, 256, batch_first=True)\n",
       "  )\n",
       "  (h_activ): ReLU()\n",
       "  (out_activ): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0345,  ..., 0.0198, 0.0200, 0.0106],\n",
       "        [0.0000, 0.0000, 0.0345,  ..., 0.0191, 0.0192, 0.0110],\n",
       "        [0.0000, 0.0000, 0.0344,  ..., 0.0197, 0.0198, 0.0106],\n",
       "        ...,\n",
       "        [0.0000, 0.0000, 0.0344,  ..., 0.0196, 0.0197, 0.0107],\n",
       "        [0.0000, 0.0000, 0.0345,  ..., 0.0199, 0.0200, 0.0106],\n",
       "        [0.0000, 0.0000, 0.0344,  ..., 0.0193, 0.0194, 0.0108]],\n",
       "       grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1034, 1])\n"
     ]
    }
   ],
   "source": [
    "z, x_hat = model(x)\n",
    "\n",
    "# FIXME problem with dimensions in decoder's output. probably x_hat needs to be reshaped like [batch, len, fetaures]. currently it's as [batch * len, features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 256])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8272, 1])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 1])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.decoder.dense_matrix.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
